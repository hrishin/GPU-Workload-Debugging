apiVersion: v1
kind: Pod
metadata:
  name: gpu-textgen
  namespace: default
spec:
  restartPolicy: Never
  securityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  containers:
  - name: textgen
    image: pytorch/pytorch:2.3.1-cuda12.1-cudnn8-runtime
    imagePullPolicy: IfNotPresent
    securityContext:
      allowPrivilegeEscalation: false
      runAsNonRoot: true
      capabilities:
        drop: ["ALL"]
    resources:
      limits:
        nvidia.com/gpu: "1"
        memory: "8Gi"
        cpu: "2"
      requests:
        nvidia.com/gpu: "1"
        memory: "4Gi"
        cpu: "1"
    env:
    - name: MODEL_ID
      value: Qwen/Qwen2.5-0.5B-Instruct
    - name: PROMPT
      value: 'In one or two sentences, explain why Kubernetes GPU debugging matters:'
    - name: OUTPUT_PATH
      value: /outputs/textgen.txt
    - name: PYTHONUNBUFFERED
      value: "1"
    - name: HF_HOME
      value: /tmp/huggingface
    - name: TRANSFORMERS_CACHE
      value: /tmp/transformers_cache
    volumeMounts:
    - mountPath: /outputs
      name: out
    - mountPath: /tmp
      name: tmp-storage
    command: ["/bin/bash", "-lc"]
    args:
    - |
      set -e
      
      echo "=== GPU Text Generation Starting ==="
      echo "Timestamp: Sun Sep 28 09:44:37 UTC 2025"
      echo "Model: "
      echo "User: root"
      
      # GPU check
      echo "=== GPU Status ==="
      if command -v nvidia-smi >/dev/null; then
        nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader
      else
        echo "Warning: nvidia-smi not found"
      fi
      
      # Install packages with retries
      echo "=== Installing Dependencies ==="
      for attempt in 1 2 3; do
        echo "Install attempt /3"
        if python -m pip -q install --upgrade pip --timeout=60 &&            python -m pip -q install transformers accelerate safetensors sentencepiece tiktoken --timeout=60; then
          echo "✓ Packages installed successfully"
          break
        else
          echo "✗ Install failed, attempt "
          if [  -eq 3 ]; then
            echo "ERROR: Failed to install packages after 3 attempts"
            exit 1
          fi
          sleep 10
        fi
      done
      
      # Verify installation
      echo "=== Verifying Installation ==="
      python -c "
      import torch, transformers
      print(f'PyTorch: {torch.__version__}')
      print(f'Transformers: {transformers.__version__}')
      print(f'CUDA available: {torch.cuda.is_available()}')
      if torch.cuda.is_available():
          print(f'GPU name: {torch.cuda.get_device_name()}')
          print(f'GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB')
      "
      
      echo "=== Running Text Generation ==="
      python - <<'PY'
      import os, torch, traceback, time
      from transformers import AutoTokenizer, AutoModelForCausalLM
      
      try:
          model_id = os.getenv("MODEL_ID")
          prompt = os.getenv("PROMPT")  
          out_path = os.getenv("OUTPUT_PATH")
          
          print(f"Loading model: {model_id}")
          start_time = time.time()
          
          # Verify CUDA
          assert torch.cuda.is_available(), "CUDA not available"
          device = "cuda"
          dtype = torch.float16
          
          # Load tokenizer and model
          tok = AutoTokenizer.from_pretrained(model_id)
          if tok.pad_token is None:
              tok.pad_token = tok.eos_token
              
          model = AutoModelForCausalLM.from_pretrained(
              model_id, 
              torch_dtype=dtype, 
              low_cpu_mem_usage=True,
              device_map="auto"
          )
          
          load_time = time.time() - start_time
          print(f"Model loaded in {load_time:.1f}s")
          
          # Show GPU memory usage
          if torch.cuda.is_available():
              memory_gb = torch.cuda.memory_allocated() / 1e9
              print(f"GPU memory used: {memory_gb:.1f}GB")
          
          # Generate text
          print("Generating text...")
          x = tok(prompt, return_tensors="pt").to(device)
          
          with torch.no_grad():
              y = model.generate(
                  **x,
                  max_new_tokens=64,
                  do_sample=True,
                  temperature=0.7,
                  top_p=0.9,
                  repetition_penalty=1.15,
                  no_repeat_ngram_size=3,
                  eos_token_id=tok.eos_token_id,
                  pad_token_id=tok.eos_token_id
              )
          
          text = tok.decode(y[0], skip_special_tokens=True)
          
          # Save output
          os.makedirs(os.path.dirname(out_path), exist_ok=True)
          with open(out_path, "w", encoding="utf-8") as f:
              f.write(f"# Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
              f.write(f"# Model: {model_id}\n") 
              f.write(f"# Load time: {load_time:.1f}s\n\n")
              f.write(text + "\n")
          
          print(f"✓ SUCCESS: {out_path}")
          print(f"Generated text: {text}")
          
      except Exception as e:
          print(f"✗ ERROR: {e}")
          traceback.print_exc()
          exit(1)
      PY
      
      echo "=== Job Complete ==="
      echo "Output file info:"
      if [ -f "" ]; then
        ls -la ""
        echo "File size:  bytes"
      else
        echo "ERROR: Output file not created"
        exit 1
      fi
      
  dnsPolicy: ClusterFirst
  runtimeClassName: nvidia
  serviceAccount: default
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: out
    hostPath:
      path: /var/lib/gpu-text-outputs
      type: DirectoryOrCreate
  - name: tmp-storage
    emptyDir:
      sizeLimit: "1Gi"
